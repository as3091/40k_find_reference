{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7326ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorv/conda/envs/40k_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "genai.__version__\n",
    "\n",
    "# import chromadb\n",
    "#from chromadb import Documents, EmbeddingFunction, Embeddings , Client\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from ebooklib import epub\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1383d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254d6add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n"
     ]
    }
   ],
   "source": [
    "for m in client.models.list():\n",
    "    if \"embedContent\" in m.supported_actions:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f56fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chapter_Book(body_soup):\n",
    "    \"\"\"Extract text from paragraphs with Chapter_Book class\"\"\"\n",
    "    chapter_book_paras = body_soup.find_all('p', class_='Chapter_Book')\n",
    "    texts = []\n",
    "    for para in chapter_book_paras:\n",
    "        text = para.get_text(strip=True)\n",
    "        if text:  # Only add non-empty text\n",
    "            texts.append(text)\n",
    "            # print(f\"Chapter_Book: {text}\")\n",
    "    return \"\".join(texts)\n",
    "\n",
    "# def Chapter_Quote(body_soup):\n",
    "#     \"\"\"Extract text from paragraphs with Chapter_Quote class\"\"\"\n",
    "#     all_paras = body_soup.find_all('p')\n",
    "#     chapter_quote_paras = []\n",
    "    \n",
    "#     for para in all_paras:\n",
    "#         para_classes = para.get('class', [])\n",
    "#         # Check if any class contains \"Quote\"\n",
    "#         if any('Quote' in cls for cls in para_classes):\n",
    "#             chapter_quote_paras.append(para)\n",
    "#     texts = []\n",
    "#     for para in chapter_quote_paras:\n",
    "#         # print(chapter_quote_paras)\n",
    "#         text = para.get_text(strip=True)\n",
    "#         if text:  # Only add non-empty text\n",
    "#             texts.append(text)\n",
    "#             # print(f\"Chapter_Quote: {text}\\n\")\n",
    "#     return \" \".join(texts)\n",
    "\n",
    "def Subtitle(body_soup):\n",
    "    \"\"\"Extract text from paragraphs with Subtitle class\"\"\"\n",
    "    subtitle_paras = body_soup.find_all('p', class_='Subtitle')\n",
    "    texts = []\n",
    "    for para in subtitle_paras:\n",
    "        text = para.get_text(strip=True)\n",
    "        if text:  # Only add non-empty text\n",
    "            texts.append(text)\n",
    "            # print(f\"Subtitle: {text}\\n\")\n",
    "    return \"\".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc9226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epub_to_documents(epub_path):\n",
    "    book = epub.read_epub(epub_path)\n",
    "    documents = []\n",
    "    for item in book.get_items():\n",
    "        if \"content\" in item.get_name().lower():\n",
    "            chapter_content = item.content.decode('utf-8')\n",
    "            soup = BeautifulSoup(chapter_content, 'html.parser')\n",
    "            body_soup = soup.body\n",
    "            body_text = body_soup.get_text(separator='\\n', strip=True)\n",
    "            metadata = {}\n",
    "\n",
    "            # classes_to_find = ['Chapter_Book', 'class_']\n",
    "            if body_soup.find_all('p', class_='Chapter_Book'):\n",
    "                metadata['Type'] = 'Chapter'\n",
    "                metadata['Chapter_Name'] = Chapter_Book(body_soup)\n",
    "                metadata['Chapter_Subtitle'] = Subtitle(body_soup)\n",
    "\n",
    "            elif body_soup.find_all('p', class_='Chapter_Quote'):\n",
    "                metadata['Type'] = 'Quote'\n",
    "            \n",
    "            lines = (line.strip() for line in body_text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            cleaned_text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "            if cleaned_text:\n",
    "                doc = Document(\n",
    "                    page_content=cleaned_text,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe39343",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517d9e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: My_Black_Library/Leviathan-eBook-Eng-2023.epub\n",
      "Processed 48 chapters/documents from the book.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorv/conda/envs/40k_env/lib/python3.13/site-packages/ebooklib/epub.py:1347: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"My_Black_Library\"\n",
    "for root, dirs, files in os.walk(persist_directory):\n",
    "    # root: current directory path\n",
    "    # dirs: list of subdirectory names in root\n",
    "    # files: list of file names in root\n",
    "    for file in files:\n",
    "        if file.endswith(\".epub\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            documents_from_epub = epub_to_documents(file_path)\n",
    "            print(f\"Processed {len(documents_from_epub)} chapters/documents from the book.\")\n",
    "            break\n",
    "            book = epub.read_epub(os.path.join(root, file))\n",
    "            for item in book.get_items():\n",
    "                if item.get_type() == epub.ITEM_DOCUMENT:  # This is a chapter\n",
    "                    print(f\"Chapter ID: {item.id}, Title: {item.title}\")\n",
    "                    # You can then process the content of the chapter:\n",
    "                    # chapter_content = item.content.decode('utf-8')\n",
    "                    # print(chapter_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43bf5585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the book into 765 chunks with metadata.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=150\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents_from_epub)\n",
    "print(f\"Split the book into {len(chunks)} chunks with metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2deabb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata[\"chunk_location\"] = f\"Chunk_{i+1}_of_{len(chunks)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86fc17af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings and creating ChromaDB store...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating embeddings and creating ChromaDB store...\")\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6fdda07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and saved to 'My_Black_Library'.\n"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(\n",
    "    chunks, \n",
    "    embedding_model, \n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "print(f\"Vector store created and saved to '{persist_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84b8249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_db = db\n",
    "# loaded_db = Chroma(\n",
    "#     persist_directory=persist_directory, \n",
    "#     embedding_function=embedding_model\n",
    "# )\n",
    "\n",
    "# Perform a similarity search\n",
    "query = \"What is the main character's motivation?\"\n",
    "results = loaded_db.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e922635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top results for query: 'What is the main character's motivation?'\n",
      "\n",
      "--- Result 1 ---\n",
      "**Content**: ‘This is different. You know it is. We’re not just facing an angry mob this time.’ He rested his forehead on hers. Then he loosed her hand and walked over to the table. ‘Why in the name of the Emperor did they choose me as governor? I’ve never sought...\n",
      "\n",
      "--- Result 2 ---\n",
      "**Content**: greatest\n",
      "strength? He thought back over everything he had read, from Guilliman’s masterwork, the Codex Astartes, to other military texts and obscure meditations on the vagaries of the warp. He realised that, for once, he could not easily answer. Seve...\n",
      "\n",
      "--- Result 3 ---\n",
      "**Content**: Abarim relaxed as his mind settled on the correct answer. ‘My greatest strength is reason. The power to make a choice. My body could be broken. My etheric powers could be nulled. Tactics can fail. But whatever befalls me, I will always have the power...\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTop results for query: '{query}'\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    # print(f\"**Chapter**: {doc.metadata.get('chapter_title', 'N/A')}\")\n",
    "    # print(f\"**Location**: {doc.metadata.get('chunk_location', 'N/A')}\")\n",
    "    print(f\"**Content**: {doc.page_content[:250]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19be110",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c541427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "\n",
    "class GeminiEmbeddingFunction(chromadb.EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries\n",
    "    document_mode = True\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    def __call__(self, input: chromadb.Documents) -> chromadb.Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        response = client.models.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            contents=input,\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type=embedding_task,\n",
    "            ),\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8144597",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"Apoorv_40k_find_reference_db\"\n",
    "\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "embed_fn.document_mode = True\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "40k_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
